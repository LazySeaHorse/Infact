{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still cooking lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "You'll need to restart your runtime when it prompts you to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Processing Pipeline\n",
    "# ============================================\n",
    "# INSTALLATION AND SETUP CELL\n",
    "# ============================================\n",
    "\n",
    "!pip install -q streamlit sentence-transformers scikit-learn gensim spacy textblob fuzzywuzzy python-Levenshtein\n",
    "!pip install -q google-generativeai langchain langchain-google-genai pyngrok wordcloud plotly\n",
    "!pip install -q nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m textblob.download_corpora\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# NLP and ML\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim import corpora, models\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LLM and orchestration\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Tunneling\n",
    "from pyngrok import ngrok\n",
    "from google.colab import userdata\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Set up ngrok\n",
    "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
    "ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"])\n",
    "\n",
    "# Set up Gemini API\n",
    "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ============================================\n",
    "# MAIN APPLICATION CODE\n",
    "# ============================================\n",
    "\n",
    "# Save as app.py\n",
    "app_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import required libraries\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim import corpora, models\n",
    "from fuzzywuzzy import fuzz\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Configure page\n",
    "st.set_page_config(\n",
    "    page_title=\"Infact\",\n",
    "    page_icon=\"üì∞\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main {padding: 0rem 1rem;}\n",
    "    .stAlert {margin-top: 1rem;}\n",
    "    h1 {color: #1E3A8A;}\n",
    "    h2 {color: #2563EB;}\n",
    "    h3 {color: #3B82F6;}\n",
    "    .metric-card {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        color: white;\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Title and description\n",
    "st.title(\"üì∞ Infact\")\n",
    "st.markdown(\"**Desensationalizing news through AI-powered clustering and fact extraction**\")\n",
    "\n",
    "# Initialize session state\n",
    "if 'processed_data' not in st.session_state:\n",
    "    st.session_state.processed_data = None\n",
    "if 'current_stage' not in st.session_state:\n",
    "    st.session_state.current_stage = 0\n",
    "\n",
    "# Cache model loading\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_models():\n",
    "    with st.spinner(\"Loading AI models... (one-time setup)\"):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        if torch.cuda.is_available():\n",
    "            sentence_model = sentence_model.to('cuda')\n",
    "        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))\n",
    "        return nlp, sentence_model\n",
    "\n",
    "nlp, sentence_model = load_models()\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def preprocess_text(text, nlp):\n",
    "    \"\"\"Clean and preprocess text using spaCy\"\"\"\n",
    "    doc = nlp(text[:1000000])  # Limit for memory\n",
    "    tokens = [token.lemma_.lower() for token in doc\n",
    "              if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def extract_embeddings(texts, model, batch_size=32):\n",
    "    \"\"\"Extract embeddings with batch processing\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def cluster_articles(embeddings, texts, n_clusters=None):\n",
    "    \"\"\"Cluster articles using TF-IDF + KMeans\"\"\"\n",
    "    if n_clusters is None:\n",
    "        n_clusters = min(max(3, len(texts) // 20), 15)\n",
    "\n",
    "    # TF-IDF for feature enhancement\n",
    "    tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "    tfidf_features = tfidf.fit_transform(texts).toarray()\n",
    "\n",
    "    # Combine embeddings with TF-IDF\n",
    "    combined_features = np.hstack([embeddings, tfidf_features * 0.3])\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(combined_features)\n",
    "\n",
    "    return clusters, kmeans\n",
    "\n",
    "def name_clusters_lda(texts, clusters, n_topics=1):\n",
    "    \"\"\"Name clusters using LDA topic modeling\"\"\"\n",
    "    cluster_names = {}\n",
    "\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        cluster_texts = [texts[i] for i in range(len(texts)) if clusters[i] == cluster_id]\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = [text.split() for text in cluster_texts]\n",
    "\n",
    "        # Create dictionary and corpus\n",
    "        dictionary = corpora.Dictionary(tokenized)\n",
    "        corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
    "\n",
    "        # LDA model\n",
    "        if len(corpus) > 0:\n",
    "            lda = models.LdaModel(\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=n_topics,\n",
    "                random_state=42,\n",
    "                passes=10,\n",
    "                alpha='auto'\n",
    "            )\n",
    "\n",
    "            # Get top words\n",
    "            topics = lda.show_topics(num_topics=n_topics, num_words=5, formatted=False)\n",
    "            if topics:\n",
    "                words = [word for word, _ in topics[0][1]]\n",
    "                cluster_names[cluster_id] = \" \".join(words[:3]).title()\n",
    "            else:\n",
    "                cluster_names[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "        else:\n",
    "            cluster_names[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "\n",
    "    return cluster_names\n",
    "\n",
    "def extract_facts_and_musings(text, nlp):\n",
    "    \"\"\"Extract fact bullets and separate musings using NER and rules\"\"\"\n",
    "    doc = nlp(text[:5000])  # Limit for processing\n",
    "\n",
    "    facts = []\n",
    "    musings = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if not sent_text:\n",
    "            continue\n",
    "\n",
    "        # Use TextBlob for sentiment\n",
    "        blob = TextBlob(sent_text)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "\n",
    "        # Check for entities (facts often contain entities)\n",
    "        has_entities = len(sent.ents) > 0\n",
    "\n",
    "        # Check for opinion indicators\n",
    "        opinion_words = ['believe', 'think', 'feel', 'seems', 'appears', 'might', 'could', 'should', 'opinion']\n",
    "        has_opinion = any(word in sent_text.lower() for word in opinion_words)\n",
    "\n",
    "        # Check for factual indicators\n",
    "        fact_indicators = ['reported', 'announced', 'confirmed', 'according to', 'data shows', 'study']\n",
    "        has_fact_indicator = any(indicator in sent_text.lower() for indicator in fact_indicators)\n",
    "\n",
    "        # Classification logic\n",
    "        if has_fact_indicator or (has_entities and not has_opinion and abs(sentiment) < 0.5):\n",
    "            # Clean up the fact\n",
    "            fact = sent_text.replace('\\\\n', ' ').strip()\n",
    "            if len(fact) > 20 and len(fact) < 300:  # Reasonable length for a bullet\n",
    "                facts.append(fact)\n",
    "        elif has_opinion or abs(sentiment) > 0.6:\n",
    "            musing = sent_text.replace('\\\\n', ' ').strip()\n",
    "            if len(musing) > 20 and len(musing) < 300:\n",
    "                musings.append(musing)\n",
    "\n",
    "    return facts[:10], musings[:5]  # Limit number of bullets\n",
    "\n",
    "def merge_similar_bullets(bullets, threshold=0.7):\n",
    "    \"\"\"Merge similar bullets using cosine similarity and fuzzy matching\"\"\"\n",
    "    if len(bullets) <= 1:\n",
    "        return bullets, []\n",
    "\n",
    "    # Calculate similarity matrix\n",
    "    merged = []\n",
    "    used = set()\n",
    "    similarity_scores = []\n",
    "\n",
    "    for i, bullet1 in enumerate(bullets):\n",
    "        if i in used:\n",
    "            continue\n",
    "\n",
    "        similar_group = [bullet1]\n",
    "        for j, bullet2 in enumerate(bullets[i+1:], i+1):\n",
    "            if j in used:\n",
    "                continue\n",
    "\n",
    "            # Fuzzy matching\n",
    "            ratio = fuzz.token_sort_ratio(bullet1, bullet2) / 100.0\n",
    "\n",
    "            if ratio > threshold:\n",
    "                similar_group.append(bullet2)\n",
    "                used.add(j)\n",
    "                similarity_scores.append(ratio)\n",
    "\n",
    "        # Keep the longest bullet from similar group\n",
    "        merged.append(max(similar_group, key=len))\n",
    "        used.add(i)\n",
    "\n",
    "    return merged, similarity_scores\n",
    "\n",
    "def generate_article_with_gemini(bullets, musings, cluster_name):\n",
    "    \"\"\"Generate desensationalized article using Gemini API\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "        bullets_text = \" ‚Ä¢ \".join(bullets) if bullets else \"No specific facts available.\"\n",
    "        musings_text = \" ‚Ä¢ \".join(musings) if musings else \"No opinions or commentary available.\"\n",
    "\n",
    "        prompt = f\"\"\"Generate a desensationalized news article about {cluster_name} from these merged fact bullets:\n",
    "\n",
    "FACTS:\n",
    "{bullets_text}\n",
    "\n",
    "Write in neutral, factual journalistic style. Stick to the facts provided.\n",
    "After the main article, add a brief \"Commentary & Analysis\" section with these musings:\n",
    "{musings_text}\n",
    "\n",
    "Format the output with clear sections and professional structure.\"\"\"\n",
    "\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating article: {str(e)}\"\n",
    "\n",
    "# ============================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================\n",
    "\n",
    "# Sidebar for input\n",
    "with st.sidebar:\n",
    "    st.header(\"üì• Input Articles\")\n",
    "\n",
    "    # Sample data generator\n",
    "    if st.button(\"Generate Sample Data\"):\n",
    "        sample_data = [\n",
    "  {\"title\":\"You Won‚Äôt Believe What Governor Silverstone Is Hiding!\",\"content\":\"BREAKING: Progressive Beacon Daily has uncovered SHOCKING evidence that Governor Silverstone‚Äôs secret offshore accounts teem with illicit payoffs from corporate lobbyists! Documents obtained by our insider reveal hidden transactions totaling MILLIONS funneled through shell companies. Critics say this could spell the end of his political career. If you care about TRANSPARENCY, you NEED to read this expos√© before it‚Äôs buried forever!\"},\n",
    "  {\"title\":\"Is Silverstone the Most Corrupt Governor Ever?\",\"content\":\"In a STUNNING revelation, Centrist Times reports Governor Silverstone‚Äôs office allegedly processed suspicious wire transfers linked to big-energy giants. Sources claim these funds influenced critical environmental votes. Lawmakers are demanding a full inquiry‚Äîcould this be the biggest scandal in state history? Our exclusive analysis breaks down every transaction and political ramification. You won‚Äôt believe how deep this rabbit hole goes!\"},\n",
    "  {\"title\":\"Expose: Silverstone‚Äôs Shady Deals Threaten Our Values\",\"content\":\"Conservative Watchdog News warns that Governor Silverstone‚Äôs dereliction of duty isn‚Äôt just immoral‚Äîit‚Äôs PATRIOTIC BETRAYAL! Leaked financial ledgers allegedly show collaboration with radical green groups aiming to dismantle traditional industries. Experts fear these payoffs will cost thousands of jobs and undermine national security. Lawmakers are mobilizing to strip him of office. Don‚Äôt miss this fiery breakdown of treasonous politics!\"},\n",
    "  {\"title\":\"Incredible Breakthrough: Scientists Harness Sunlight Like Never Before!\",\"content\":\"Progressive Beacon Daily celebrates a MIND-BLOWING invention: researchers at Meridian Institute have developed solar panels that convert 90% of sunlight into energy! This could CRUSH the fossil fuel industry and save the planet. Testing shows devices working under low-light conditions‚ÄîEVERY home can go green. Environmentalists call it the TECHNOLOGY of the century. Find out how this revolution could slash your bills to zero!\"},\n",
    "  {\"title\":\"Solar Miracle Poised to Rewire Energy Market\",\"content\":\"Centrist Times reports that a team at Meridian Institute unveiled ultra-efficient solar cells boosting energy conversion rates by 50%. Investors are already lining up to fund mass production. Officials say this could stabilize electricity prices and reduce carbon emissions dramatically. Our experts break down what this means for everyday consumers and the global energy landscape. Could this be the energy shift we‚Äôve all waited for?\"},\n",
    "  {\"title\":\"New Solar Tech Sparks Fears of Industrial Collapse\",\"content\":\"Conservative Watchdog News ALERT: Meridian Institute‚Äôs latest solar innovation threatens to DESTROY American manufacturing! Reports indicate the technology could decimate traditional energy sectors, costing millions of jobs. Critics argue the government will FORCE companies to adopt this UNTESTED system, undermining economic stability. Industry leaders are mobilizing to resist‚Äîread our fiery take on how radical science is on track to wreck livelihoods.\"},\n",
    "  {\"title\":\"Hollywood‚Äôs Biggest Star Files for Divorce‚ÄîMUST-SEE Details!\",\"content\":\"Progressive Beacon Daily exposes the intimate details behind A-list actor Jordan Calibre‚Äôs shocking divorce filing from indie director Riley West. Sources say Calibre cited ‚Äúirreconcilable creative differences,‚Äù but rumors of infidelity swirl like wildfire! Friends claim West discovered damning text messages. Our exclusive interviews delve into every tearful confrontation and trust-shattering betrayal, plus what it means for Calibre‚Äôs upcoming blockbuster release.\"},\n",
    "  {\"title\":\"Celebrity Split Shocks Fans Worldwide\",\"content\":\"Centrist Times reveals actor Jordan Calibre has petitioned for divorce from Riley West after a decade-long marriage. While the pair released a joint statement emphasizing mutual respect, insiders hint at deep artistic disagreements and financial disputes. We break down the timeline of their relationship, the terms of their prenuptial agreement, and what this could mean for their sprawling media empire.\"},\n",
    "  {\"title\":\"Star Divorce: Hollywood‚Äôs Moral Decay Exposed\",\"content\":\"Conservative Watchdog News decries Jordan Calibre‚Äôs divorce from Riley West as yet another symbol of Hollywood‚Äôs crumbling moral fabric. Sources allege West‚Äôs radical ideology clashed with Calibre‚Äôs family values, prompting this public split. Experts warn this trend undermines societal cohesion. Our explosive report uncovers behind-the-scenes drama, lavish alimony demands, and the culture-war stakes at play in Tinseltown‚Äôs latest breakup.\"},\n",
    "  {\"title\":\"SKY ALERT: Mysterious Comet Heads Straight for Earth!\",\"content\":\"Progressive Beacon Daily warns: NASA scientists have detected Comet Talora hurtling toward Earth at BREAKNECK speed! Groundbreaking telescopes estimate a collision chance of 2%. While experts urge calm, conspiracy theorists speculate involvement of secret government satellites. Will we see a celestial spectacle‚Äîor total annihilation? Our live updates and expert interviews guide you through every astronomical twist before it‚Äôs too late!\"},\n",
    "  {\"title\":\"Comet Talora: Real Risk or Media Circus?\",\"content\":\"Centrist Times highlights recent NASA data on Comet Talora, currently 70 million km away and tracking a near-Earth trajectory. Officials place the impact probability at less than 1%, forecasting a dazzling sky show rather than disaster. We clarify scientific jargon, weigh expert assessments, and outline safe viewing protocols. Learn what the public should REALLY know amid the swirling cosmic hype.\"},\n",
    "  {\"title\":\"Armageddon Incoming? Comet Talora Doom Predictions!\",\"content\":\"Conservative Watchdog News screams ALERT: Comet Talora might be God‚Äôs final judgment on a morally bankrupt world! Prepper communities stockpile supplies as the celestial object grows ominously bright. Though NASA insists there‚Äôs ‚Äúno cause for alarm,‚Äù regional pastors call for national prayer days. Could this be the sign we‚Äôve ignored for too long? Discover how this cosmic visitor might expose society‚Äôs spiritual failings!\"},\n",
    "  {\"title\":\"Hospital Crisis: ERs Drowning in Patients‚ÄîMUST READ!\",\"content\":\"Progressive Beacon Daily uncovers a nationwide EMERGENCY as public hospitals report 200% ER capacity surges amid unprecedented flu and COVID-variant outbreaks. Frontline nurses sound the alarm on staff shortages and dwindling medical supplies. Patients wait HOURS for care. Health advocates demand major funding overhauls to save lives. Our exclusive testimonies reveal heartbreaking stories behind overcrowded wards and the real human cost you won‚Äôt believe!\"},\n",
    "  {\"title\":\"ER Overload: What You Need to Know\",\"content\":\"Centrist Times examines the current strain on emergency departments across the country, attributing it to overlapping flu, COVID-19, and RSV seasons. Hospitals report bed shortages and extended wait times. Officials propose federal grants and rapid staffing incentives to alleviate pressure. We analyze policy options, compare regional responses, and provide practical tips for seeking timely medical attention during the crisis.\"},\n",
    "  {\"title\":\"Hospitals on Brink: Government Failures EXPOSED\",\"content\":\"Conservative Watchdog News BLASTS federal mandates for causing ER meltdowns, with hospitals forced to treat unlawful migrants and non-citizens, leaving locals to suffer. Staff report shutdown threats if they refuse care. Citizens face life-or-death delays while bureaucrats bicker. This is a TAXPAYER SCANDAL! Our fiery investigation names the officials responsible and outlines the radical reforms needed to save American healthcare.\"},\n",
    "  {\"title\":\"School‚Äôs New AI Pronoun Rules Spark Controversy!\",\"content\":\"Progressive Beacon Daily reveals TechForward Academy‚Äôs radical introduction of AI-driven pronoun enforcement‚Äîa digital system that auto-corrects speech for inclusivity! Students are seeing real-time alerts and mandatory sensitivity training. Advocates hail it as the FUTURE of respect; critics decry Orwellian overreach. Our in-depth look explores student reactions, privacy concerns, and the impact on classroom culture you can‚Äôt afford to miss.\"},\n",
    "  {\"title\":\"AI Pronoun Tool: Balanced Perspectives\",\"content\":\"Centrist Times reports that TechForward Academy is piloting an AI pronoun-assistant to promote inclusivity. The system flags misgendering and offers immediate guidance. Supporters argue it fosters empathy, while detractors question data security and free speech implications. We present viewpoints from educators, legal experts, and parent groups, plus a side-by-side analysis of the program‚Äôs benefits and potential pitfalls.\"},\n",
    "  {\"title\":\"School‚Äôs AI Pronoun Police: Free Speech Under Siege?\",\"content\":\"Conservative Watchdog News warns TechForward Academy‚Äôs AI pronoun enforcement is the latest step toward THOUGHT CONTROL in schools! Students risk penalties for ‚Äòunintentional‚Äô speech errors, and faculty face dismissal if they push back. This techno-authoritarian nightmare could spread nationwide, stifling dissent and bending education to radical ideology. Read our blistering critique on how AI is weaponized against fundamental liberties!\"}\n",
    "]\n",
    "        st.session_state.sample_json = json.dumps(sample_data, indent=2)\n",
    "\n",
    "    articles_json = st.text_area(\n",
    "        \"Paste JSON array of articles:\",\n",
    "        value=st.session_state.get('sample_json', ''),\n",
    "        height=300,\n",
    "        help='Format: [{\"title\": \"...\", \"content\": \"...\"}, ...]'\n",
    "    )\n",
    "\n",
    "    process_button = st.button(\"üöÄ Process Articles\", type=\"primary\", use_container_width=True)\n",
    "\n",
    "# Main content area with tabs\n",
    "if process_button and articles_json:\n",
    "    try:\n",
    "        articles = json.loads(articles_json)\n",
    "        st.session_state.articles = articles\n",
    "        st.session_state.current_stage = 1\n",
    "\n",
    "        # Create tabs for different stages\n",
    "        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([\n",
    "            \"1Ô∏è‚É£ Preprocessing\",\n",
    "            \"2Ô∏è‚É£ Clustering\",\n",
    "            \"3Ô∏è‚É£ Topic Extraction\",\n",
    "            \"4Ô∏è‚É£ Fact & Musing Extraction\",\n",
    "            \"5Ô∏è‚É£ Deduplication\",\n",
    "            \"6Ô∏è‚É£ Final Articles\"\n",
    "        ])\n",
    "\n",
    "        # Tab 1: Preprocessing\n",
    "        with tab1:\n",
    "            st.header(\"üìã Data Preprocessing\")\n",
    "\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.metric(\"Total Articles\", len(articles))\n",
    "            with col2:\n",
    "                avg_length = np.mean([len(a['content']) for a in articles])\n",
    "                st.metric(\"Avg Article Length\", f\"{avg_length:.0f} chars\")\n",
    "            with col3:\n",
    "                st.metric(\"Processing Time\", \"< 1 min\")\n",
    "\n",
    "            with st.spinner(\"Cleaning and preprocessing text...\"):\n",
    "                processed_texts = []\n",
    "                original_texts = []\n",
    "                titles = []\n",
    "\n",
    "                progress_bar = st.progress(0)\n",
    "                for i, article in enumerate(articles):\n",
    "                    combined_text = article['title'] + \" \" + article['content']\n",
    "                    processed = preprocess_text(combined_text, nlp)\n",
    "                    processed_texts.append(processed)\n",
    "                    original_texts.append(combined_text)\n",
    "                    titles.append(article['title'])\n",
    "                    progress_bar.progress((i + 1) / len(articles))\n",
    "\n",
    "                st.success(f\"‚úÖ Preprocessed {len(processed_texts)} articles\")\n",
    "\n",
    "                # Show sample\n",
    "                with st.expander(\"View Sample Preprocessed Text\"):\n",
    "                    st.text(\"Original:\")\n",
    "                    st.write(original_texts[0][:500])\n",
    "                    st.text(\"Processed:\")\n",
    "                    st.write(processed_texts[0][:500])\n",
    "\n",
    "        # Tab 2: Clustering\n",
    "        with tab2:\n",
    "            st.header(\"üéØ Article Clustering\")\n",
    "\n",
    "            with st.spinner(\"Generating embeddings and clustering...\"):\n",
    "                # Generate embeddings\n",
    "                embeddings = extract_embeddings(original_texts, sentence_model)\n",
    "\n",
    "                # Cluster\n",
    "                n_clusters = st.slider(\"Number of clusters:\", 3, 15, min(7, len(articles)//3))\n",
    "                clusters, kmeans_model = cluster_articles(embeddings, processed_texts, n_clusters)\n",
    "\n",
    "                # Visualize clusters\n",
    "                col1, col2 = st.columns(2)\n",
    "\n",
    "                with col1:\n",
    "                    # Cluster distribution\n",
    "                    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "                    fig_dist = px.bar(\n",
    "                        x=cluster_counts.index,\n",
    "                        y=cluster_counts.values,\n",
    "                        labels={'x': 'Cluster ID', 'y': 'Number of Articles'},\n",
    "                        title=\"Cluster Size Distribution\"\n",
    "                    )\n",
    "                    st.plotly_chart(fig_dist, use_container_width=True, key=\"cluster_distribution_chart\")\n",
    "\n",
    "                with col2:\n",
    "                    # PCA visualization\n",
    "                    pca = PCA(n_components=2)\n",
    "                    embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "                    fig_pca = px.scatter(\n",
    "                        x=embeddings_2d[:, 0],\n",
    "                        y=embeddings_2d[:, 1],\n",
    "                        color=clusters,\n",
    "                        title=\"Cluster Visualization (PCA)\",\n",
    "                        labels={'x': 'Component 1', 'y': 'Component 2'},\n",
    "                        color_continuous_scale='Viridis'\n",
    "                    )\n",
    "                    st.plotly_chart(fig_pca, use_container_width=True, key=\"pca_visualization_chart\")\n",
    "\n",
    "                st.success(f\"‚úÖ Created {n_clusters} story clusters\")\n",
    "\n",
    "        # Tab 3: Topic Extraction\n",
    "        with tab3:\n",
    "            st.header(\"üè∑Ô∏è Cluster Naming via Topic Modeling\")\n",
    "\n",
    "            with st.spinner(\"Extracting topics with LDA...\"):\n",
    "                cluster_names = name_clusters_lda(processed_texts, clusters)\n",
    "\n",
    "                # Display cluster names with word clouds\n",
    "                cols = st.columns(3)\n",
    "                for idx, (cluster_id, name) in enumerate(cluster_names.items()):\n",
    "                    with cols[idx % 3]:\n",
    "                        st.subheader(f\"Cluster {cluster_id}: {name}\")\n",
    "\n",
    "                        # Get texts for this cluster\n",
    "                        cluster_texts = [processed_texts[i] for i in range(len(processed_texts))\n",
    "                                       if clusters[i] == cluster_id]\n",
    "\n",
    "                        # Generate mini word cloud\n",
    "                        if cluster_texts:\n",
    "                            text_combined = \" \".join(cluster_texts)\n",
    "                            wordcloud = WordCloud(width=300, height=200, background_color='white').generate(text_combined)\n",
    "\n",
    "                            fig, ax = plt.subplots(figsize=(4, 3))\n",
    "                            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "                            ax.axis('off')\n",
    "                            st.pyplot(fig)\n",
    "                            plt.close()\n",
    "\n",
    "                            st.caption(f\"Articles: {len(cluster_texts)}\")\n",
    "\n",
    "        # Tab 4: Fact & Musing Extraction\n",
    "        with tab4:\n",
    "            st.header(\"üìù Fact Bullet & Musing Extraction\")\n",
    "\n",
    "            all_cluster_data = {}\n",
    "\n",
    "            for cluster_id in range(n_clusters):\n",
    "                cluster_articles = [original_texts[i] for i in range(len(original_texts))\n",
    "                                  if clusters[i] == cluster_id]\n",
    "\n",
    "                if not cluster_articles:\n",
    "                    continue\n",
    "\n",
    "                with st.expander(f\"Cluster {cluster_id}: {cluster_names.get(cluster_id, 'Unknown')}\"):\n",
    "                    all_facts = []\n",
    "                    all_musings = []\n",
    "\n",
    "                    # Extract from each article in cluster\n",
    "                    for article in cluster_articles[:10]:  # Limit for speed\n",
    "                        facts, musings = extract_facts_and_musings(article, nlp)\n",
    "                        all_facts.extend(facts)\n",
    "                        all_musings.extend(musings)\n",
    "\n",
    "                    col1, col2 = st.columns(2)\n",
    "\n",
    "                    with col1:\n",
    "                        st.subheader(\"üìå Extracted Facts\")\n",
    "                        for i, fact in enumerate(all_facts[:5], 1):\n",
    "                            st.write(f\"{i}. {fact}\")\n",
    "                        st.metric(\"Total Facts Extracted\", len(all_facts))\n",
    "\n",
    "                    with col2:\n",
    "                        st.subheader(\"üí≠ Extracted Musings\")\n",
    "                        for i, musing in enumerate(all_musings[:5], 1):\n",
    "                            st.write(f\"{i}. {musing}\")\n",
    "                        st.metric(\"Total Musings Extracted\", len(all_musings))\n",
    "\n",
    "                    all_cluster_data[cluster_id] = {\n",
    "                        'facts': all_facts,\n",
    "                        'musings': all_musings,\n",
    "                        'name': cluster_names.get(cluster_id, f\"Cluster {cluster_id}\")\n",
    "                    }\n",
    "\n",
    "        # Tab 5: Deduplication\n",
    "        with tab5:\n",
    "            st.header(\"üîÑ Bullet Deduplication & Merging\")\n",
    "\n",
    "            for cluster_id, data in all_cluster_data.items():\n",
    "                with st.expander(f\"Cluster {cluster_id}: {data['name']}\"):\n",
    "                    original_facts = data['facts']\n",
    "                    merged_facts, similarity_scores = merge_similar_bullets(original_facts)\n",
    "\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    with col1:\n",
    "                        st.metric(\"Original Facts\", len(original_facts))\n",
    "                    with col2:\n",
    "                        st.metric(\"After Merging\", len(merged_facts))\n",
    "                    with col3:\n",
    "                        reduction = (1 - len(merged_facts)/max(len(original_facts), 1)) * 100\n",
    "                        st.metric(\"Reduction\", f\"{reduction:.1f}%\")\n",
    "\n",
    "                    if similarity_scores:\n",
    "                        # Similarity heatmap\n",
    "                        st.subheader(\"Similarity Distribution\")\n",
    "                        fig_sim = px.histogram(\n",
    "                            similarity_scores,\n",
    "                            nbins=20,\n",
    "                            labels={'value': 'Similarity Score', 'count': 'Frequency'},\n",
    "                            title=\"Distribution of Similarity Scores\"\n",
    "                        )\n",
    "                        st.plotly_chart(fig_sim, use_container_width=True, key=f\"similarity_histogram_{cluster_id}\")\n",
    "\n",
    "                    # Update data with merged facts\n",
    "                    data['merged_facts'] = merged_facts\n",
    "\n",
    "        # Tab 6: Final Articles\n",
    "        with tab6:\n",
    "            st.header(\"üì∞ Desensationalized Articles\")\n",
    "\n",
    "            for cluster_id, data in all_cluster_data.items():\n",
    "                st.subheader(f\"Story: {data['name']}\")\n",
    "\n",
    "                with st.spinner(f\"Generating article for {data['name']}...\"):\n",
    "                    article = generate_article_with_gemini(\n",
    "                        data.get('merged_facts', data['facts']),\n",
    "                        data['musings'],\n",
    "                        data['name']\n",
    "                    )\n",
    "\n",
    "                    # Display in a nice format\n",
    "                    with st.container():\n",
    "                        st.markdown(f\"\"\"\n",
    "                        <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
    "                            <h3 style=\"color: #1E3A8A;\">{data['name']}</h3>\n",
    "                            <div style=\"margin-top: 15px; line-height: 1.6;\">\n",
    "                                {article.replace(chr(10), '<br>')}\n",
    "                            </div>\n",
    "                            <div style=\"margin-top: 15px; padding-top: 15px; border-top: 1px solid #ddd;\">\n",
    "                                <small style=\"color: #666;\">\n",
    "                                    üìä Based on {len(data.get('merged_facts', []))} fact bullets from {sum(1 for c in clusters if c == cluster_id)} articles\n",
    "                                </small>\n",
    "                            </div>\n",
    "                        </div>\n",
    "                        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "                st.divider()\n",
    "\n",
    "            # Summary metrics\n",
    "            st.success(\"‚úÖ Processing Complete!\")\n",
    "            col1, col2, col3, col4 = st.columns(4)\n",
    "            with col1:\n",
    "                st.metric(\"Total Articles Processed\", len(articles))\n",
    "            with col2:\n",
    "                st.metric(\"Story Clusters Created\", n_clusters)\n",
    "            with col3:\n",
    "                total_facts = sum(len(d.get('merged_facts', [])) for d in all_cluster_data.values())\n",
    "                st.metric(\"Total Facts Extracted\", total_facts)\n",
    "            with col4:\n",
    "                st.metric(\"Final Articles Generated\", len(all_cluster_data))\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        st.error(\"‚ùå Invalid JSON format. Please check your input.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    # Landing page\n",
    "    st.info(\"üëà Please paste your article JSON in the sidebar and click 'Process Articles' to begin.\")\n",
    "\n",
    "    # Demo overview\n",
    "    st.header(\"üéØ Pipeline Overview\")\n",
    "\n",
    "    pipeline_steps = [\n",
    "        (\"1Ô∏è‚É£ Input\", \"Paste JSON array of news articles\"),\n",
    "        (\"2Ô∏è‚É£ Preprocess\", \"Clean text using spaCy NLP\"),\n",
    "        (\"3Ô∏è‚É£ Embed & Cluster\", \"Group similar stories using sentence transformers + KMeans\"),\n",
    "        (\"4Ô∏è‚É£ Name Clusters\", \"Extract topics using LDA topic modeling\"),\n",
    "        (\"5Ô∏è‚É£ Extract Facts\", \"Separate facts from opinions using NER + rules\"),\n",
    "        (\"6Ô∏è‚É£ Deduplicate\", \"Merge similar bullets using fuzzy matching\"),\n",
    "        (\"7Ô∏è‚É£ Generate\", \"Create neutral articles using Gemini AI\")\n",
    "    ]\n",
    "\n",
    "    cols = st.columns(len(pipeline_steps))\n",
    "    for i, (step, desc) in enumerate(pipeline_steps):\n",
    "        with cols[i]:\n",
    "            st.markdown(f\"**{step}**\")\n",
    "            st.caption(desc)\n",
    "\n",
    "    # Key features\n",
    "    st.header(\"‚ú® Key Features\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"\"\"\n",
    "        **ü§ñ Advanced AI Stack**\n",
    "        - Sentence Transformers for embeddings\n",
    "        - Multi-stage clustering pipeline\n",
    "        - LDA topic modeling\n",
    "        - Gemini 1.5 for generation\n",
    "        \"\"\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"\"\"\n",
    "        **üìä Rich Visualizations**\n",
    "        - Cluster distribution charts\n",
    "        - PCA embeddings plot\n",
    "        - Word clouds for topics\n",
    "        - Similarity heatmaps\n",
    "        \"\"\")\n",
    "\n",
    "    with col3:\n",
    "        st.markdown(\"\"\"\n",
    "        **‚ö° Production Ready**\n",
    "        - Batch processing for scale\n",
    "        - GPU acceleration support\n",
    "        - Efficient deduplication\n",
    "        - Real-time processing metrics\n",
    "        \"\"\")\n",
    "'''\n",
    "\n",
    "# Write the app file\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"‚úÖ App file created successfully!\")\n",
    "\n",
    "# ============================================\n",
    "# LAUNCH STREAMLIT WITH NGROK\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "def run_streamlit():\n",
    "    \"\"\"Run Streamlit app in background\"\"\"\n",
    "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
    "\n",
    "# Start Streamlit in background thread\n",
    "thread = Thread(target=run_streamlit)\n",
    "thread.daemon = True\n",
    "thread.start()\n",
    "\n",
    "# Wait for Streamlit to start\n",
    "time.sleep(5)\n",
    "\n",
    "# Create tunnel\n",
    "public_url = ngrok.connect(8501)\n",
    "print(\"=\" * 50)\n",
    "print(f\"üöÄ Streamlit app is running!\")\n",
    "print(f\"üì± Public URL: {public_url}\")\n",
    "print(f\"üîó Click here to open: {public_url}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚ö†Ô∏è Note: First load may take a minute to cache models\")\n",
    "print(\"üí° Tip: Use the 'Generate Sample Data' button to test quickly\")\n",
    "\n",
    "# Keep the tunnel open\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Shutting down tunnel...\")\n",
    "    ngrok.disconnect(public_url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
